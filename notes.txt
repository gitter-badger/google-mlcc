Descending into ML
------------------
* Std Dev = sqrt( sum( (y - mean)**2 ) / N - 1 )
* Linear model in machine learning: y' = b + w1x1
    - y' is the predicted label
    - b is the bias (y intercept), sometimes w0
    - w1 is the weight of feature 1 (slope)
    - x1 is a feature (known input)
* Model with multiple features might be y' = b + w1x1 + ... + wnxn
* Error is machine learning is "loss"; Algorithm builds a model to
  minimize loss (called empirical risk minimization)
* Mean square error (MSE) = avg squared loss per observation
    1 / N * sum( (y - prediction(x))**2 )
      - (x, y) is an example, x is feature set & y is label
      - prediction(x) is a function of the weights / bias
      - N is the number of examples

Reducing Loss
-------------
* Hyperparams are params that affect machine learning rate
* Get a "direction" within parameter space
* Gradient: derivative of the loss function with respect to weights/biases
    - Tells us which direction to move in order to minimize loss
    - Think parabola with goal as local minimum
* Iterative approach:
    while loss > loss_target:
        - Data in
        - Compute gradient
        - Update model params in direction of negative gradient
        - Re-compute model
        - Check the loss w/ new model
* Small gradient will take forever to reach minimum, large gradient will overshoot
    the minimum
* Ideal "learning rate" (hyperparam for gradient step size) is 1 / f(x)'', the
    inverse of the 2nd deriv of f(x) @ x
* Stochastic Gradient Descent: Compute the gradient over one random example at a time
    - More noisy but faster than batch
* Mini-Batch Gradient Descent: Average gradient/loss of batch of 10-1000
    - Less noisy than stochastic but still more efficient than using whole dataset

First Steps with TensorFlow
---------------------------
* Pandas
    - DataFrame is a relational data table
    - Series is a single column
    - DataFrame from dict of [Column Name]-->[Series]
    - pandas.read_csv()
    - DataFrame.describe(), DataFrame.head()
    - Series.apply(func) to apply complex func to a Series, returning a mask

Generalization
--------------
* If we fit a model on a fixed dataset with 100% accuracy, there is
    danger of "overfitting", in which loss is low but new observations will be
    judged inaccurately
* How do we know if our model is good?
    - Theoretically: Measure model simplicity / complexity
    - Intuition: Is the model overly complicated?
    - Empirically:
        * Will our model do well on a new set of data?
        * Evaluation: Test on a new sample (test set); Good performance
            on the test set is indicator of good performance in general if:
                - The test set is large enough
                - We don't cheat by over-using the test set
            If training set & test set perform similarily, good idea of how
            general our model is
* Three basic assumptions:
    1) We draw observations independently & identically (i.i.d.) at random
        from the distribution
    2) The distribution is stationary
    3) We pull training, validation, test, & other sets from the same distrib

Training & Test Sets
--------------------
- Randomize, then split into training, test sets
- Large training set == better model learning
- Large test set == more confidence in model
- Don't ever train on test data: Unrealistically accurate results

Validation
----------
3rd dataset partition: Validation data, so we don't start to overfit to dataset
Iterative approach:
    - Train model on training set
    - Evaluate model on validation set
    - Tweak model according to validation set results
With the best model from the validation set, confirm results on test set
